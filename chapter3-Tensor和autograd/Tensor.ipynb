{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第三章 PyTorch基础：Tensor和Autograd\n",
    "\n",
    "## 3.1 Tensor\n",
    "\n",
    "Tensor，又名张量，读者可能对这个名词似曾相识，因它不仅在PyTorch中出现过，它也是Theano、TensorFlow、\n",
    "Torch和MxNet中重要的数据结构。关于张量的本质不乏深度的剖析，但从工程角度来讲，可简单地认为它就是一个数组，且支持高效的科学计算。它可以是一个数（标量）、一维数组（向量）、二维数组（矩阵）和更高维的数组（高阶数据）。Tensor和Numpy的ndarrays类似，但PyTorch的tensor支持GPU加速。\n",
    "\n",
    "本节将系统讲解tensor的使用，力求面面俱到，但不会涉及每个函数。对于更多函数及其用法，读者可通过在IPython/Notebook中使用函数名加`?`查看帮助文档，或查阅PyTorch官方文档[^1]。\n",
    "\n",
    "[^1]: http://docs.pytorch.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1.3.1'"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's begin\n",
    "from __future__ import print_function\n",
    "import torch  as t\n",
    "t.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  3.1.1 基础操作\n",
    "\n",
    "学习过Numpy的读者会对本节内容感到非常熟悉，因tensor的接口有意设计成与Numpy类似，以方便用户使用。但不熟悉Numpy也没关系，本节内容并不要求先掌握Numpy。\n",
    "\n",
    "从接口的角度来讲，对tensor的操作可分为两类：\n",
    "\n",
    "1. `torch.function`，如`torch.save`等。\n",
    "2. 另一类是`tensor.function`，如`tensor.view`等。\n",
    "\n",
    "为方便使用，对tensor的大部分操作同时支持这两类接口，在本书中不做具体区分，如`torch.sum (torch.sum(a, b))`与`tensor.sum (a.sum(b))`功能等价。\n",
    "\n",
    "而从存储的角度来讲，对tensor的操作又可分为两类：\n",
    "\n",
    "1. 不会修改自身的数据，如 `a.add(b)`， 加法的结果会返回一个新的tensor。\n",
    "2. 会修改自身的数据，如 `a.add_(b)`， 加法的结果仍存储在a中，a被修改了。\n",
    "\n",
    "函数名以`_`结尾的都是inplace方式, 即会修改调用者自己的数据，在实际应用中需加以区分。\n",
    "\n",
    "#### 创建Tensor\n",
    "\n",
    "在PyTorch中新建tensor的方法有很多，具体如表3-1所示。\n",
    "\n",
    "表3-1: 常见新建tensor的方法\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|Tensor(\\*sizes)|基础构造函数|\n",
    "|tensor(data,)|类似np.array的构造函数|\n",
    "|ones(\\*sizes)|全1Tensor|\n",
    "|zeros(\\*sizes)|全0Tensor|\n",
    "|eye(\\*sizes)|对角线为1，其他为0|\n",
    "|arange(s,e,step|从s到e，步长为step|\n",
    "|linspace(s,e,steps)|从s到e，均匀切分成steps份|\n",
    "|rand/randn(\\*sizes)|均匀/标准分布|\n",
    "|normal(mean,std)/uniform(from,to)|正态分布/均匀分布|\n",
    "|randperm(m)|随机排列|\n",
    "\n",
    "这些创建方法都可以在创建的时候指定数据类型dtype和存放device(cpu/gpu).\n",
    "\n",
    "\n",
    "其中使用`Tensor`函数新建tensor是最复杂多变的方式，它既可以接收一个list，并根据list的数据新建tensor，也能根据指定的形状新建tensor，还能传入其他的tensor，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1.3563e-19, 9.3205e-09, 1.8034e+17],\n        [4.1588e+21, 1.9789e+37, 9.1124e-12]])"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 指定tensor的形状\n",
    "a = t.Tensor(2, 3)\n",
    "a # 数值取决于内存空间的状态，print时候可能overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 2., 3.],\n        [4., 5., 6.]])"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用list的数据创建tensor\n",
    "b = t.Tensor([[1,2,3],[4,5,6]])\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]]"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.tolist() # 把tensor转为list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "`tensor.size()`返回`torch.Size`对象，它是tuple的子类，但其使用方式与tuple略有区别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_size = b.size()\n",
    "b_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "6"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numel() # b中元素总个数，2*3，等价于b.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[1.3563e-19, 9.3205e-09, 8.4078e-45],\n         [0.0000e+00, 1.4013e-45, 0.0000e+00]]), tensor([2., 3.]))"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 创建一个和b形状一样的tensor\n",
    "c = t.Tensor(b_size)\n",
    "# 创建一个元素为2和3的tensor\n",
    "d = t.Tensor((2, 3))\n",
    "c, d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "除了`tensor.size()`，还可以利用`tensor.shape`直接查看tensor的形状，`tensor.shape`等价于`tensor.size()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "需要注意的是，`t.Tensor(*sizes)`创建tensor时，系统不会马上分配空间，只是会计算剩余的内存是否足够使用，使用到tensor时才会分配，而其它操作都是在创建完tensor之后马上进行空间分配。其它常用的创建tensor的方法举例如下。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.ones(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1, 3, 5])"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.arange(1, 6, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 1.0000,  5.5000, 10.0000])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.linspace(1, 10, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-1.9675, -2.3524, -1.1079],\n        [-0.7262, -1.4429, -0.1361]])"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randn(2, 3, device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3, 4, 2, 1, 0])"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.randperm(5) # 长度为5的随机排列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 0, 0],\n        [0, 1, 0]], dtype=torch.int32)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.eye(2, 3, dtype=t.int) # 对角线为1, 不要求行列数一致"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "`torch.tensor`是在0.4版本新增加的一个新版本的创建tensor方法，使用的方法，和参数几乎和`np.array`完全一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "scalar: tensor(3.1416), shape of sclar: torch.Size([])\n"
    }
   ],
   "source": [
    "scalar = t.tensor(3.14159) \n",
    "print('scalar: %s, shape of sclar: %s' %(scalar, scalar.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "vector: tensor([1, 2]), shape of vector: torch.Size([2])\n"
    }
   ],
   "source": [
    "vector = t.tensor([1, 2])\n",
    "print('vector: %s, shape of vector: %s' %(vector, vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 2])"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor = t.Tensor(1,2) # 注意和t.tensor([1, 2])的区别\n",
    "tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[0.1000, 1.2000],\n         [2.2000, 3.1000],\n         [4.9000, 5.2000]]), torch.Size([3, 2]))"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix = t.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])\n",
    "matrix,matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.1111, 0.2222, 0.3333]], dtype=torch.float64)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                     dtype=t.float64,\n",
    "                     device=t.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([0])"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "empty_tensor = t.tensor([])\n",
    "empty_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 常用Tensor操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "通过`tensor.view`方法可以调整tensor的形状，但必须保证调整前后元素总数一致。`view`不会修改自身的数据，返回的新tensor与源tensor共享内存，也即更改其中的一个，另外一个也会跟着改变。在实际应用中可能经常需要添加或减少某一维度，这时候`squeeze`和`unsqueeze`两个函数就派上用场了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2],\n        [3, 4, 5]])"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.view(2, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 3])"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(-1, 3) # 当某一维为-1的时候，会自动计算它的大小\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([2, 1, 3])"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(1) # 注意形状，在第1维（下标从0开始）上增加“１” \n",
    "#等价于 b[:,None]\n",
    "b[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[0, 1, 2]],\n\n        [[3, 4, 5]]])"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.unsqueeze(-2) # -2表示倒数第二个维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[[0, 1, 2],\n          [3, 4, 5]]]])"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.view(1, 1, 1, 2, 3)\n",
    "c.squeeze(0) # 压缩第0维的“１”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 1, 2],\n        [3, 4, 5]])"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.squeeze() # 把所有维度为“1”的压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0, 100,   2],\n        [  3,   4,   5]])"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[1] = 100\n",
    "b # a修改，b作为view之后的，也会跟着修改"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "`resize`是另一种可用来调整`size`的方法，但与`view`不同，它可以修改tensor的大小。如果新大小超过了原大小，会自动分配新的内存空间，而如果新大小小于原大小，则之前的数据依旧会被保存，看一个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0, 100,   2]])"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(1, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[                0,               100,                 2],\n        [                3,                 4,                 5],\n        [19703385815842913, 32370056120500329, 32088624091889756]])"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.resize_(3, 3) # 旧的数据依旧保存着，多出的大小会分配新空间\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 索引操作\n",
    "\n",
    "Tensor支持与numpy.ndarray类似的索引操作，语法上也类似，下面通过一些例子，讲解常用的索引操作。如无特殊说明，索引出来的结果与原tensor共享内存，也即修改一个，另一个会跟着修改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5.9058e-02, -4.0459e-02, -3.6815e-01, -2.6455e-01],\n        [-7.1757e-01,  1.3208e-03, -1.8390e+00,  1.7855e+00],\n        [-6.6298e-01,  2.7308e-01, -7.3029e-01, -6.4676e-01]])"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0591, -0.0405, -0.3682, -0.2646])"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0] # 第0行(下标从0开始)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0591, -0.7176, -0.6630])"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:, 0] # 第0列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.3682)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][2] # 第0行第2个元素，等价于a[0, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-0.2646)"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, -1] # 第0行最后一个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5.9058e-02, -4.0459e-02, -3.6815e-01, -2.6455e-01],\n        [-7.1757e-01,  1.3208e-03, -1.8390e+00,  1.7855e+00]])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2] # 前两行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0591, -0.0405],\n        [-0.7176,  0.0013]])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:2, 0:2] # 前两行，第0,1列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[ 0.0591, -0.0405]])\ntensor([ 0.0591, -0.0405])\n"
    }
   ],
   "source": [
    "print(a[0:1, :2]) # 第0行，前两列 \n",
    "print(a[0, :2]) # 注意两者的区别：形状不同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 4])"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# None类似于np.newaxis, 为a新增了一个轴\n",
    "# 等价于a.view(1, a.shape[0], a.shape[1])\n",
    "a[None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 3, 4])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[None].shape # 等价于a[None,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 1, 4])"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([3, 1, 4, 1, 1])"
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:,None,:,None,None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False, False, False, False],\n        [False, False, False,  True],\n        [False, False, False, False]])"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a > 1 # 返回一个ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([1.7855])"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>1] # 等价于a.masked_select(a>1)\n",
    "# 选择结果与原tensor不共享内存空间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 5.9058e-02, -4.0459e-02, -3.6815e-01, -2.6455e-01],\n        [-7.1757e-01,  1.3208e-03, -1.8390e+00,  1.7855e+00]])"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[t.LongTensor([0,1])] # 第0行和第1行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "其它常用的选择函数如表3-2所示。\n",
    "\n",
    "表3-2常用的选择函数\n",
    "\n",
    "函数|功能|\n",
    ":---:|:---:|\n",
    "index_select(input, dim, index)|在指定维度dim上选取，比如选取某些行、某些列\n",
    "masked_select(input, mask)|例子如上，a[a>0]，使用ByteTensor进行选取\n",
    "non_zero(input)|非0元素的下标\n",
    "gather(input, dim, index)|根据index，在dim维度上选取数据，输出的size与index一样\n",
    "\n",
    "\n",
    "`gather`是一个比较复杂的操作，对一个2维tensor，输出的每个元素如下：\n",
    "\n",
    "```python\n",
    "out[i][j] = input[index[i][j]][j]  # dim=0\n",
    "out[i][j] = input[i][index[i][j]]  # dim=1\n",
    "```\n",
    "三维tensor的`gather`操作同理，下面举几个例子。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15]])"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 16).view(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  5, 10, 15]])"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取对角线的元素\n",
    "index = t.LongTensor([[0,1,2,3]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 3],\n        [ 6],\n        [ 9],\n        [12]])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素\n",
    "index = t.LongTensor([[3,2,1,0]]).t()\n",
    "a.gather(1, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[12,  9,  6,  3]])"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取反对角线上的元素，注意与上面的不同\n",
    "index = t.LongTensor([[3,2,1,0]])\n",
    "a.gather(0, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0,  3],\n        [ 5,  6],\n        [10,  9],\n        [15, 12]])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选取两个对角线上的元素\n",
    "index = t.LongTensor([[0,1,2,3],[3,2,1,0]]).t()\n",
    "b = a.gather(1, index)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "与`gather`相对应的逆操作是`scatter_`，`gather`把数据从input中按index取出，而`scatter_`是把取出的数据再放回去。注意`scatter_`函数是inplace操作。\n",
    "\n",
    "```python\n",
    "out = input.gather(dim, index)\n",
    "-->近似逆操作\n",
    "out = Tensor()\n",
    "out.scatter_(dim, index)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  0.,  0.,  3.],\n        [ 0.,  5.,  6.,  0.],\n        [ 0.,  9., 10.,  0.],\n        [12.,  0.,  0., 15.]])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把两个对角线元素放回去到指定位置\n",
    "c = t.zeros(4,4)\n",
    "c.scatter_(1, index, b.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "对tensor的任何索引操作仍是一个tensor，想要获取标准的python对象数值，需要调用`tensor.item()`, 这个方法只对包含一个元素的tensor适用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(0)"
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0] #依旧是tensor）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0,0].item() # python float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "torch.Size([1, 1, 1])\n"
    },
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = a[0:1, 0:1, None]\n",
    "print(d.shape)\n",
    "d.item() # 只包含一个元素的tensor即可调用tensor.item,与形状无关"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a[0].item()  ->\n",
    "# raise ValueError: only one element tensors can be converted to Python scalars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 高级索引\n",
    "PyTorch在0.2版本中完善了索引操作，目前已经支持绝大多数numpy的高级索引[^10]。高级索引可以看成是普通索引操作的扩展，但是高级索引操作的结果一般不和原始的Tensor贡献内出。 \n",
    "[^10]: https://docs.scipy.org/doc/numpy/reference/arrays.indexing.html#advanced-indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8]],\n\n        [[ 9, 10, 11],\n         [12, 13, 14],\n         [15, 16, 17]],\n\n        [[18, 19, 20],\n         [21, 22, 23],\n         [24, 25, 26]]])"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = t.arange(0,27).view(3,3,3)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([14, 24])"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[1, 2], [1, 2], [2, 0]] # x[1,1,2]和x[2,2,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([19, 10,  1])"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[2, 1, 0], [0], [1]] # x[2,0,1],x[1,0,1],x[0,0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[ 0,  1,  2],\n         [ 3,  4,  5],\n         [ 6,  7,  8]],\n\n        [[18, 19, 20],\n         [21, 22, 23],\n         [24, 25, 26]]])"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[[0, 2], ...] # x[0] 和 x[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Tensor类型\n",
    "\n",
    "Tensor有不同的数据类型，如表3-3所示，每种类型分别对应有CPU和GPU版本(HalfTensor除外)。默认的tensor是FloatTensor，可通过`t.set_default_tensor_type` 来修改默认tensor类型(如果默认类型为GPU tensor，则所有操作都将在GPU上进行)。Tensor的类型对分析内存占用很有帮助。例如对于一个size为(1000, 1000, 1000)的FloatTensor，它有`1000*1000*1000=10^9`个元素，每个元素占32bit/8 = 4Byte内存，所以共占大约4GB内存/显存。HalfTensor是专门为GPU版本设计的，同样的元素个数，显存占用只有FloatTensor的一半，所以可以极大缓解GPU显存不足的问题，但由于HalfTensor所能表示的数值大小和精度有限[^2]，所以可能出现溢出等问题。\n",
    "\n",
    "[^2]: https://stackoverflow.com/questions/872544/what-range-of-numbers-can-be-represented-in-a-16-32-and-64-bit-ieee-754-syste\n",
    "\n",
    "表3-3: tensor数据类型\n",
    "\n",
    "| Data type                | dtype                             | CPU tensor                                                   | GPU tensor                |\n",
    "| ------------------------ | --------------------------------- | ------------------------------------------------------------ | ------------------------- |\n",
    "| 32-bit floating point    | `torch.float32` or `torch.float`  | `torch.FloatTensor`                                          | `torch.cuda.FloatTensor`  |\n",
    "| 64-bit floating point    | `torch.float64` or `torch.double` | `torch.DoubleTensor`                                         | `torch.cuda.DoubleTensor` |\n",
    "| 16-bit floating point    | `torch.float16` or `torch.half`   | `torch.HalfTensor`                                           | `torch.cuda.HalfTensor`   |\n",
    "| 8-bit integer (unsigned) | `torch.uint8`                     | [`torch.ByteTensor`](https://pytorch.org/docs/stable/tensors.html#torch.ByteTensor) | `torch.cuda.ByteTensor`   |\n",
    "| 8-bit integer (signed)   | `torch.int8`                      | `torch.CharTensor`                                           | `torch.cuda.CharTensor`   |\n",
    "| 16-bit integer (signed)  | `torch.int16` or `torch.short`    | `torch.ShortTensor`                                          | `torch.cuda.ShortTensor`  |\n",
    "| 32-bit integer (signed)  | `torch.int32` or `torch.int`      | `torch.IntTensor`                                            | `torch.cuda.IntTensor`    |\n",
    "| 64-bit integer (signed)  | `torch.int64` or `torch.long`     | `torch.LongTensor`                                           | `torch.cuda.LongTensor`   |\n",
    "\n",
    " \n",
    "\n",
    "各数据类型之间可以互相转换，`type(new_type)`是通用的做法，同时还有`float`、`long`、`half`等快捷方法。CPU tensor与GPU tensor之间的互相转换通过`tensor.cuda`和`tensor.cpu`方法实现，此外还可以使用`tensor.to(device)`。Tensor还有一个`new`方法，用法与`t.Tensor`一样，会调用该tensor对应类型的构造函数，生成与当前tensor类型一致的tensor。`torch.*_like(tensora)` 可以生成和`tensora`拥有同样属性(类型，形状，cpu/gpu)的新tensor。 `tensor.new_*(new_shape)` 新建一个不同形状的tensor。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置默认tensor，注意参数是字符串\n",
    "t.set_default_tensor_type('torch.DoubleTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float64"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor(2,3)\n",
    "a.dtype # 现在a是DoubleTensor,dtype是float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 恢复之前的默认设置\n",
    "t.set_default_tensor_type('torch.FloatTensor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 把a转成FloatTensor，等价于b=a.type(t.FloatTensor)\n",
    "b = a.float() \n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.]])"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.type_as(b)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new(2,3) # 等价于torch.DoubleTensor(2,3)，建议使用a.new_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 0., 0.],\n        [0., 0., 0.]], dtype=torch.float64)"
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a) #等价于t.zeros(a.shape,dtype=a.dtype,device=a.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0, 0, 0],\n        [0, 0, 0]], dtype=torch.int16)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.zeros_like(a, dtype=t.int16) #可以修改某些属性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0.5525, 0.7321, 0.2923],\n        [0.7667, 0.2929, 0.9311]], dtype=torch.float64)"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand_like(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1],\n        [1, 1, 1, 1, 1]], dtype=torch.int32)"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_ones(4,5, dtype=t.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3., 4.], dtype=torch.float64)"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.new_tensor([3,4]) # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 逐元素操作\n",
    "\n",
    "这部分操作会对tensor的每一个元素(point-wise，又名element-wise)进行操作，此类操作的输入与输出形状一致。常用的操作如表3-4所示。\n",
    "\n",
    "表3-4: 常见的逐元素操作\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|abs/sqrt/div/exp/fmod/log/pow..|绝对值/平方根/除法/指数/求余/求幂..|\n",
    "|cos/sin/asin/atan2/cosh..|相关三角函数|\n",
    "|ceil/round/floor/trunc| 上取整/四舍五入/下取整/只保留整数部分|\n",
    "|clamp(input, min, max)|超过min和max部分截断|\n",
    "|sigmod/tanh..|激活函数\n",
    "\n",
    "对于很多操作，例如div、mul、pow、fmod等，PyTorch都实现了运算符重载，所以可以直接使用运算符。如`a ** 2` 等价于`torch.pow(a,2)`, `a * 2`等价于`torch.mul(a,2)`。\n",
    "\n",
    "其中`clamp(x, min, max)`的输出满足以下公式：\n",
    "$$\n",
    "y_i =\n",
    "\\begin{cases}\n",
    "min,  & \\text{if  } x_i \\lt min \\\\\n",
    "x_i,  & \\text{if  } min \\le x_i \\le max  \\\\\n",
    "max,  & \\text{if  } x_i \\gt max\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "`clamp`常用在某些需要比较大小的地方，如取一个tensor的每个元素与另一个数的较大值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 1.0000,  0.5403, -0.4161],\n        [-0.9900, -0.6536,  0.2837]])"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3).float()\n",
    "t.cos(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[0., 1., 2.],\n        [0., 1., 2.]])"
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a % 3 # 等价于t.fmod(a, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  1.,  4.],\n        [ 9., 16., 25.]])"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a ** 2 # 等价于t.pow(a, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0., 1., 2.],\n        [3., 4., 5.]])\n"
    },
    {
     "data": {
      "text/plain": "tensor([[3., 3., 3.],\n        [3., 4., 5.]])"
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 取a中的每一个元素与3相比较大的一个 (小于3的截断成3)\n",
    "print(a)\n",
    "t.clamp(a, min=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.0000,  0.8415,  0.9093],\n        [ 0.1411, -0.7568, -0.9589]])"
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.sin_() # 效果同 a = a.sin();b=a ,但是更高效节省显存\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "####  归并操作 \n",
    "此类操作会使输出形状小于输入形状，并可以沿着某一维度进行指定操作。如加法`sum`，既可以计算整个tensor的和，也可以计算tensor中每一行或每一列的和。常用的归并操作如表3-5所示。\n",
    "\n",
    "表3-5: 常用归并操作\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|mean/sum/median/mode|均值/和/中位数/众数|\n",
    "|norm/dist|范数/距离|\n",
    "|std/var|标准差/方差|\n",
    "|cumsum/cumprod|累加/累乘|\n",
    "\n",
    "以上大多数函数都有一个参数**`dim`**，用来指定这些操作是在哪个维度上执行的。关于dim(对应于Numpy中的axis)的解释众说纷纭，这里提供一个简单的记忆方式：\n",
    "\n",
    "假设输入的形状是(m, n, k)\n",
    "\n",
    "- 如果指定dim=0，输出的形状就是(1, n, k)或者(n, k)\n",
    "- 如果指定dim=1，输出的形状就是(m, 1, k)或者(m, k)\n",
    "- 如果指定dim=2，输出的形状就是(m, n, 1)或者(m, n)\n",
    "\n",
    "size中是否有\"1\"，取决于参数`keepdim`，`keepdim=True`会保留维度`1`。注意，以上只是经验总结，并非所有函数都符合这种形状变化方式，如`cumsum`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[2., 2., 2.]])"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.ones(2, 3)\n",
    "b.sum(dim = 0, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([2., 2., 2.])"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keepdim=False，不保留维度\"1\"，注意形状\n",
    "b.sum(dim=0, keepdim=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([3., 3.])"
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor([[0, 1, 2],\n        [3, 4, 5]])\n"
    },
    {
     "data": {
      "text/plain": "tensor([[ 0,  1,  3],\n        [ 3,  7, 12]])"
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6).view(2, 3)\n",
    "print(a)\n",
    "a.cumsum(dim=1) # 沿着行累加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 比较\n",
    "比较函数中有一些是逐元素比较，操作类似于逐元素操作，还有一些则类似于归并操作。常用比较函数如表3-6所示。\n",
    "\n",
    "表3-6: 常用比较函数\n",
    "\n",
    "|函数|功能|\n",
    "|:--:|:--:|\n",
    "|gt/lt/ge/le/eq/ne|大于/小于/大于等于/小于等于/等于/不等|\n",
    "|topk|最大的k个数|\n",
    "|sort|排序|\n",
    "|max/min|比较两个tensor最大最小值|\n",
    "\n",
    "表中第一行的比较操作已经实现了运算符重载，因此可以使用`a>=b`、`a>b`、`a!=b`、`a==b`，其返回结果是一个`ByteTensor`，可用来选取元素。max/min这两个操作比较特殊，以max来说，它有以下三种使用情况：\n",
    "- t.max(tensor)：返回tensor中最大的一个数\n",
    "- t.max(tensor,dim)：指定维上最大的数，返回tensor和下标\n",
    "- t.max(tensor1, tensor2): 比较两个tensor相比较大的元素\n",
    "\n",
    "至于比较一个tensor和一个数，可以使用clamp函数。下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  3.,  6.],\n        [ 9., 12., 15.]])"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.linspace(0, 15, 6).view(2, 3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[15., 12.,  9.],\n        [ 6.,  3.,  0.]])"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.linspace(15, 0, 6).view(2, 3)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[False, False, False],\n        [ True,  True,  True]])"
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a>b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 9., 12., 15.])"
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>b] # a中大于b的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(15.)"
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.return_types.max(\nvalues=tensor([15.,  6.]),\nindices=tensor([0, 0]))"
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(b, dim=1) \n",
    "# 第一个返回值的15和6分别表示第0行和第1行最大的元素\n",
    "# 第二个返回值的0和0表示上述最大的数是该行第0个元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[15., 12.,  9.],\n        [ 9., 12., 15.]])"
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.max(a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[10., 10., 10.],\n        [10., 12., 15.]])"
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 比较a和10较大的元素\n",
    "t.clamp(a, min=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 线性代数\n",
    "\n",
    "PyTorch的线性函数主要封装了Blas和Lapack，其用法和接口都与之类似。常用的线性代数函数如表3-7所示。\n",
    "\n",
    "表3-7: 常用的线性代数函数\n",
    "\n",
    "|函数|功能|\n",
    "|:---:|:---:|\n",
    "|trace|对角线元素之和(矩阵的迹)|\n",
    "|diag|对角线元素|\n",
    "|triu/tril|矩阵的上三角/下三角，可指定偏移量|\n",
    "|mm/bmm|矩阵乘法，batch的矩阵乘法|\n",
    "|addmm/addbmm/addmv/addr/badbmm..|矩阵运算\n",
    "|t|转置|\n",
    "|dot/cross|内积/外积\n",
    "|inverse|求逆矩阵\n",
    "|svd|奇异值分解\n",
    "\n",
    "具体使用说明请参见官方文档[^3]，需要注意的是，矩阵的转置会导致存储空间不连续，需调用它的`.contiguous`方法将其转为连续。\n",
    "[^3]: http://pytorch.org/docs/torch.html#blas-and-lapack-operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.t()\n",
    "b.is_contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.,  9.],\n        [ 3., 12.],\n        [ 6., 15.]])"
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1.2 Tensor和Numpy\n",
    "\n",
    "Tensor和Numpy数组之间具有很高的相似性，彼此之间的互操作也非常简单高效。需要注意的是，Numpy和Tensor共享内存。由于Numpy历史悠久，支持丰富的操作，所以当遇到Tensor不支持的操作时，可先转成Numpy数组，处理后再转回tensor，其转换开销很小。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1., 1., 1.],\n       [1., 1., 1.]], dtype=float32)"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "a = np.ones([2, 3],dtype=np.float32)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 也可以直接将numpy对象传入Tensor\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  1., 100.,   1.],\n        [  1.,   1.,   1.]])"
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1]=100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  1., 100.,   1.],\n       [  1.,   1.,   1.]], dtype=float32)"
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = b.numpy() # a, b, c三个对象共享内存\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**注意**： 当numpy的数据类型和Tensor的类型不一样的时候，数据会被复制，不会共享内存。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dtype('float64')"
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.ones([2, 3])\n",
    "# 注意和上面的a的区别（dtype不是float32）\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "torch.float32"
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = t.Tensor(a) # 此处进行拷贝，不共享内存\n",
    "b.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.]], dtype=torch.float64)"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = t.from_numpy(a) # 注意c的类型（DoubleTensor）\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[1., 1., 1.],\n        [1., 1., 1.]])"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0, 1] = 100\n",
    "b # b与a不共享内存，所以即使a改变了，b也不变"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  1., 100.,   1.],\n        [  1.,   1.,   1.]], dtype=torch.float64)"
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c # c与a共享内存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**注意：** 不论输入的类型是什么，t.tensor都会进行数据拷贝，不会共享内存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = t.tensor(a) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[  1., 100.,   1.],\n       [  1.,   1.,   1.]])"
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor[0,0]=0\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "广播法则(broadcast)是科学运算中经常使用的一个技巧，它在快速执行向量化的同时不会占用额外的内存/显存。\n",
    "Numpy的广播法则定义如下：\n",
    "\n",
    "- 让所有输入数组都向其中shape最长的数组看齐，shape中不足的部分通过在前面加1补齐\n",
    "- 两个数组要么在某一个维度的长度一致，要么其中一个为1，否则不能计算 \n",
    "- 当输入数组的某个维度的长度为1时，计算时沿此维度复制扩充成一样的形状\n",
    "\n",
    "PyTorch当前已经支持了自动广播法则，但是笔者还是建议读者通过以下两个函数的组合手动实现广播法则，这样更直观，更不易出错：\n",
    "\n",
    "- `unsqueeze`或者`view`，或者tensor[None],：为数据某一维的形状补1，实现法则1\n",
    "- `expand`或者`expand_as`，重复数组，实现法则3；该操作不会复制数组，所以不会占用额外的空间。\n",
    "\n",
    "注意，repeat实现与expand相类似的功能，但是repeat会把相同数据复制多份，因此会占用额外的空间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = t.ones(3, 2)\n",
    "b = t.zeros(2, 3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 1.],\n         [1., 1.],\n         [1., 1.]],\n\n        [[1., 1.],\n         [1., 1.],\n         [1., 1.]]])"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 自动广播法则\n",
    "# 第一步：a是2维,b是3维，所以先在较小的a前面补1 ，\n",
    "#               即：a.unsqueeze(0)，a的形状变成（1，3，2），b的形状是（2，3，1）,\n",
    "# 第二步:   a和b在第一维和第三维形状不一样，其中一个为1 ，\n",
    "#               可以利用广播法则扩展，两个形状都变成了（2，3，2）\n",
    "a+b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[[1., 1.],\n         [1., 1.],\n         [1., 1.]],\n\n        [[1., 1.],\n         [1., 1.],\n         [1., 1.]]])"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 手动广播法则\n",
    "# 或者 a.view(1,3,2).expand(2,3,2)+b.expand(2,3,2)\n",
    "a[None].expand(2, 3, 2) + b.expand(2,3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand不会占用额外空间，只会在需要的时候才扩充，可极大节省内存\n",
    "e = a.unsqueeze(0).expand(10000000000000, 3,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1.3 内部结构\n",
    "\n",
    "tensor的数据结构如图3-1所示。tensor分为头信息区(Tensor)和存储区(Storage)，信息区主要保存着tensor的形状（size）、步长（stride）、数据类型（type）等信息，而真正的数据则保存成连续数组。由于数据动辄成千上万，因此信息区元素占用内存较少，主要内存占用则取决于tensor中元素的数目，也即存储区的大小。\n",
    "\n",
    "一般来说一个tensor有着与之相对应的storage, storage是在data之上封装的接口，便于使用，而不同tensor的头信息一般不同，但却可能使用相同的数据。下面看两个例子。\n",
    "\n",
    "![图3-1: Tensor的数据结构](imgs/tensor_data_structure.svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": " 0\n 1\n 2\n 3\n 4\n 5\n[torch.LongStorage of size 6]"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 6)\n",
    "a.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": " 0\n 1\n 2\n 3\n 4\n 5\n[torch.LongStorage of size 6]"
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(2, 3)\n",
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 一个对象的id值可以看作它在内存中的地址\n",
    "# storage的内存地址一样，即是同一个storage\n",
    "id(b.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  0, 100,   2],\n        [  3,   4,   5]])"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a改变，b也随之改变，因为他们共享storage\n",
    "a[1] = 100\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": " 0\n 100\n 2\n 3\n 4\n 5\n[torch.LongStorage of size 6]"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a[2:] \n",
    "c.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1910852101904, 1910852101888)"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.data_ptr(), a.data_ptr() # data_ptr返回tensor首元素的内存地址\n",
    "# 可以看出相差8，这是因为2*4=8--相差两个元素，每个元素占4个字节(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([   0,  100, -100,    3,    4,    5])"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c[0] = -100 # c[0]的内存地址对应a[2]的内存地址\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[   0,  100, -100],\n        [   3,    4,    5]])"
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = t.Tensor(c.storage().float())\n",
    "d[0] = 6666\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下面４个tensor共享storage\n",
    "id(a.storage()) == id(b.storage()) == id(c.storage()) == id(d.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(0, 2, 0)"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.storage_offset(), c.storage_offset(), d.storage_offset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = b[::2, ::2] # 隔2行/列取一个元素\n",
    "id(e.storage()) == id(a.storage())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "((3, 1), (6, 2))"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.stride(), e.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.is_contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "可见绝大多数操作并不修改tensor的数据，而只是修改了tensor的头信息。这种做法更节省内存，同时提升了处理速度。在使用中需要注意。\n",
    "此外有些操作会导致tensor不连续，这时需调用`tensor.contiguous`方法将它们变成连续的数据，该方法会使数据复制一份，不再与原来的数据共享storage。\n",
    "另外读者可以思考一下，之前说过的高级索引一般不共享stroage，而普通索引共享storage，这是为什么？（提示：普通索引可以通过只修改tensor的offset，stride和size，而不修改storage来实现）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1.4 其它有关Tensor的话题\n",
    "这部分的内容不好专门划分一小节，但是笔者认为仍值得读者注意，故而将其放在这一小节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GPU/CPU\n",
    "tensor可以很随意的在gpu/cpu上传输。使用`tensor.cuda(device_id)`或者`tensor.cpu()`。另外一个更通用的方法是`tensor.to(device)`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(3, 4)\n",
    "a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = t.randn(3,4, device=t.device('cuda:0'))\n",
    "    # 等价于\n",
    "    # a.t.randn(3,4).cuda(1)\n",
    "    # 但是前者更快\n",
    "    a.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.3316,  0.5974,  0.6270, -0.1716],\n        [ 0.1607, -0.1198,  0.4217, -0.1184],\n        [ 0.8214,  1.5592,  0.1618, -0.4515]])"
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = t.device('cpu')\n",
    "a.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "**注意**\n",
    "- 尽量使用`tensor.to(device)`, 将`device`设为一个可配置的参数，这样可以很轻松的使程序同时兼容GPU和CPU\n",
    "- 数据在GPU之中传输的速度要远快于内存(CPU)到显存(GPU), 所以尽量避免频繁的在内存和显存中传输数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 持久化\n",
    "Tensor的保存和加载十分的简单，使用t.save和t.load即可完成相应的功能。在save/load时可指定使用的`pickle`模块，在load时还可将GPU tensor映射到CPU或其它GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t.cuda.is_available():\n",
    "    a = a.cuda(0) # 把a转为GPU0上的tensor,\n",
    "    t.save(a,'a.pth')\n",
    "\n",
    "    # 加载为b, 存储于GPU1上(因为保存时tensor就在GPU1上)\n",
    "    b = t.load('a.pth')\n",
    "    # 加载为c, 存储于CPU\n",
    "    c = t.load('a.pth', map_location=lambda storage, loc: storage)\n",
    "    # 加载为d, 存储于GPU0上\n",
    "    d = t.load('a.pth', map_location={'cuda:1':'cuda:0'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "####   向量化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "向量化计算是一种特殊的并行计算方式，相对于一般程序在同一时间只执行一个操作的方式，它可在同一时间执行多个操作，通常是对不同的数据执行同样的一个或一批指令，或者说把指令应用于一个数组/向量上。向量化可极大提高科学运算的效率，Python本身是一门高级语言，使用很方便，但这也意味着很多操作很低效，尤其是`for`循环。在科学计算程序中应当极力避免使用Python原生的`for循环`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def for_loop_add(x, y):\n",
    "    result = []\n",
    "    for i,j in zip(x, y):\n",
    "        result.append(i + j)\n",
    "    return t.Tensor(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "1.02 ms ± 50 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\nThe slowest run took 9.47 times longer than the fastest. This could mean that an intermediate result is being cached.\n8.49 µs ± 11.3 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
    }
   ],
   "source": [
    "x = t.zeros(100)\n",
    "y = t.ones(100)\n",
    "%timeit -n 10 for_loop_add(x, y)\n",
    "%timeit -n 10 x + y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "可见二者有超过几十倍的速度差距，因此在实际使用中应尽量调用内建函数(buildin-function)，这些函数底层由C/C++实现，能通过执行底层优化实现高效计算。因此在平时写代码时，就应养成向量化的思维习惯，千万避免对较大的tensor进行逐元素遍历。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "此外还有以下几点需要注意：\n",
    "- 大多数`t.function`都有一个参数`out`，这时候产生的结果将保存在out指定tensor之中。\n",
    "- `t.set_num_threads`可以设置PyTorch进行CPU多线程并行计算时候所占用的线程数，这个可以用来限制PyTorch所占用的CPU数目。\n",
    "- `t.set_printoptions`可以用来设置打印tensor时的数值精度和格式。\n",
    "下面举例说明。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "tensor(19999999)tensor(19999998)\n"
    },
    {
     "data": {
      "text/plain": "(tensor(19999999), tensor(19999998))"
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.arange(0, 20000000)\n",
    "print(a[-1], a[-2]) # 32bit的IntTensor精度有限导致溢出\n",
    "b = t.LongTensor()\n",
    "t.arange(0, 20000000, out=b) # 64bit的LongTensor不会溢出\n",
    "b[-1],b[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0365, -0.0904,  1.1309],\n        [ 0.9588, -0.6508, -2.1033]])"
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.randn(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[-0.0364766046, -0.0904197320,  1.1309310198],\n        [ 0.9588218331, -0.6508266926, -2.1032783985]])"
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.set_printoptions(precision=10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3.1.5 小试牛刀：线性回归"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "线性回归是机器学习入门知识，应用十分广泛。线性回归利用数理统计中回归分析，来确定两种或两种以上变量间相互依赖的定量关系的，其表达形式为$y = wx+b+e$，$e$为误差服从均值为0的正态分布。首先让我们来确认线性回归的损失函数：\n",
    "$$\n",
    "loss = \\sum_i^N \\frac 1 2 ({y_i-(wx_i+b)})^2\n",
    "$$\n",
    "然后利用随机梯度下降法更新参数$\\textbf{w}$和$\\textbf{b}$来最小化损失函数，最终学得$\\textbf{w}$和$\\textbf{b}$的数值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython import display\n",
    "\n",
    "device = t.device('cpu') #如果你想用gpu，改成t.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置随机数种子，保证在不同电脑上运行时下面的输出一致\n",
    "t.manual_seed(1000) \n",
    "\n",
    "def get_fake_data(batch_size=8):\n",
    "    ''' 产生随机数据：y=x*2+3，加上了一些噪声'''\n",
    "    x = t.rand(batch_size, 1, device=device) * 5\n",
    "    y = x * 2 + 3 +  t.randn(batch_size, 1, device=device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "<matplotlib.collections.PathCollection at 0x1bcea28ae48>"
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 363.253219 248.518125\" width=\"363.253219pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 363.253219 248.518125 \r\nL 363.253219 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\nL 355.3625 7.2 \r\nL 20.5625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"md66efcfaf6\" style=\"stroke:#1f77b4;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#pa1758d7313)\">\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"163.482594\" xlink:href=\"#md66efcfaf6\" y=\"147.250061\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"306.223625\" xlink:href=\"#md66efcfaf6\" y=\"35.480466\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.997287\" xlink:href=\"#md66efcfaf6\" y=\"95.554686\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"133.979102\" xlink:href=\"#md66efcfaf6\" y=\"136.726932\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"140.94464\" xlink:href=\"#md66efcfaf6\" y=\"129.226131\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"312.307941\" xlink:href=\"#md66efcfaf6\" y=\"43.812023\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"222.620865\" xlink:href=\"#md66efcfaf6\" y=\"72.977747\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"185.936157\" xlink:href=\"#md66efcfaf6\" y=\"97.616944\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"180.219715\" xlink:href=\"#md66efcfaf6\" y=\"75.036136\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"298.270367\" xlink:href=\"#md66efcfaf6\" y=\"28.580602\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"182.005836\" xlink:href=\"#md66efcfaf6\" y=\"129.949409\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"241.847651\" xlink:href=\"#md66efcfaf6\" y=\"62.751127\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"213.252941\" xlink:href=\"#md66efcfaf6\" y=\"79.079893\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"69.697945\" xlink:href=\"#md66efcfaf6\" y=\"214.353695\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"339.276072\" xlink:href=\"#md66efcfaf6\" y=\"17.486305\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"36.648928\" xlink:href=\"#md66efcfaf6\" y=\"175.91111\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"me0420a6f55\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"57.412849\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0.5 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n       <path d=\"M 10.6875 12.40625 \r\nL 21 12.40625 \r\nL 21 0 \r\nL 10.6875 0 \r\nz\r\n\" id=\"DejaVuSans-46\"/>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(49.461287 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.860984\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1.0 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(97.909421 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"154.309118\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 1.5 -->\r\n      <g transform=\"translate(146.357556 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"202.757253\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 2.0 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(194.80569 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"251.205387\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 2.5 -->\r\n      <g transform=\"translate(243.253825 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"299.653522\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 3.0 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(291.70196 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"348.101657\" xlink:href=\"#me0420a6f55\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 3.5 -->\r\n      <g transform=\"translate(340.150094 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m32d56bca66\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"206.042791\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 3 -->\r\n      <g transform=\"translate(7.2 209.842009)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"176.857389\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 180.656607)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"147.671987\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(7.2 151.471205)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"118.486584\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 122.285803)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"89.301182\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 7 -->\r\n      <defs>\r\n       <path d=\"M 8.203125 72.90625 \r\nL 55.078125 72.90625 \r\nL 55.078125 68.703125 \r\nL 28.609375 0 \r\nL 18.3125 0 \r\nL 43.21875 64.59375 \r\nL 8.203125 64.59375 \r\nz\r\n\" id=\"DejaVuSans-55\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 93.100401)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-55\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"60.11578\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 63.914999)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"20.5625\" xlink:href=\"#m32d56bca66\" y=\"30.930378\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 9 -->\r\n      <defs>\r\n       <path d=\"M 10.984375 1.515625 \r\nL 10.984375 10.5 \r\nQ 14.703125 8.734375 18.5 7.8125 \r\nQ 22.3125 6.890625 25.984375 6.890625 \r\nQ 35.75 6.890625 40.890625 13.453125 \r\nQ 46.046875 20.015625 46.78125 33.40625 \r\nQ 43.953125 29.203125 39.59375 26.953125 \r\nQ 35.25 24.703125 29.984375 24.703125 \r\nQ 19.046875 24.703125 12.671875 31.3125 \r\nQ 6.296875 37.9375 6.296875 49.421875 \r\nQ 6.296875 60.640625 12.9375 67.421875 \r\nQ 19.578125 74.21875 30.609375 74.21875 \r\nQ 43.265625 74.21875 49.921875 64.515625 \r\nQ 56.59375 54.828125 56.59375 36.375 \r\nQ 56.59375 19.140625 48.40625 8.859375 \r\nQ 40.234375 -1.421875 26.421875 -1.421875 \r\nQ 22.703125 -1.421875 18.890625 -0.6875 \r\nQ 15.09375 0.046875 10.984375 1.515625 \r\nz\r\nM 30.609375 32.421875 \r\nQ 37.25 32.421875 41.125 36.953125 \r\nQ 45.015625 41.5 45.015625 49.421875 \r\nQ 45.015625 57.28125 41.125 61.84375 \r\nQ 37.25 66.40625 30.609375 66.40625 \r\nQ 23.96875 66.40625 20.09375 61.84375 \r\nQ 16.21875 57.28125 16.21875 49.421875 \r\nQ 16.21875 41.5 20.09375 36.953125 \r\nQ 23.96875 32.421875 30.609375 32.421875 \r\nz\r\n\" id=\"DejaVuSans-57\"/>\r\n      </defs>\r\n      <g transform=\"translate(7.2 34.729597)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-57\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 20.5625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 355.3625 224.64 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 20.5625 224.64 \r\nL 355.3625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 20.5625 7.2 \r\nL 355.3625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pa1758d7313\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"20.5625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 来看看产生的x-y分布\n",
    "x, y = get_fake_data(batch_size=16)\n",
    "plt.scatter(x.squeeze().cpu().numpy(), y.squeeze().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deXzU9b3v8dc3ewhLWAMkhCSssqNBlCVQXEBARcW1WutCOPe0nm7XVrsc23PaamvbU+/tvY8mIlbqvtWlnrrcqkyCgCQCCgIqk5ANCFsIIfvM9/6RIAhZJpnJzGTm/Xw8eEiGmfl9Mg945+v39/l+v8Zai4iI9H4RgS5ARER8Q4EuIhIiFOgiIiFCgS4iEiIU6CIiISLKnxcbMmSITUtL8+clRUR6vcLCwsPW2qGdPc+vgZ6WlkZBQYE/Lyki0usZY/Z58jxNuYiIhAgFuohIiFCgi4iECL/OoYuI+NMrW8t5+K09VFTVMTIxnnsXT2DFzORAl9VjFOgiEpJe2VrO/S9/Ql2TC4Dyqjruf/kTgJANdU25iEhIevitPV+G+Sl1TS4efmtPgCrqeQp0EQlJFVV1XXo8FCjQRSQkjUyM79Ljwerzgyc8fq4CXURC0r2LJxAfHfmVx+KjI7l38YQAVeQ5ay2bnUe48y9buOy/HB6/TjdFRSQknbrx2Zu6XFxuy9s7D/Bnh5PtpVUMSojhe5eO57u/8ez1xp8nFmVmZlot/RcR+ar6JhcvFpaxJs9J8ZFaRg/uw6r5Gay8IIW46EiMMYXW2szO3kcjdBGRADl2spG/btrHEx8Uc+RkI9NTBvB/v34+iycPJzLCdPn9Og10Y8xaYDlQaa2d0vrYw8CVQCOwF7jDWlvV5auLiISh0qO1PJZfxHNbSqlrcrFo4jCyszKYnT4IY7oe5Kd4MkL/C/AnYN0Zj70D3G+tbTbG/Aa4H/hRt6sQEQkDO8qPk+tw8sYn+4kwcPWMZLKzMhif1M8n799poFtrHcaYtLMee/uMLzcBK31SjYhIiLHWkvf5YXIdTvK/OEzf2CjunpfOHXPTGT4gzqfX8sUc+p3Ac+39oTEmG8gGSE1N9cHlRESCX5PLzRsf7yfH4WTX/mqS+sdy3xUTuWV2Kv3jonvkml4FujHmJ0Az8FR7z7HW5gK50NLl4s31RESC3cmGZp7bUspj+UWUV9UxblhffrtyGlfPGElsVGTnb+CFbge6MeZ2Wm6WXmL92fsoIhKEDp1o4IkPivnrpn0cr2viwvRB/MfVk/nahGFEdKNjpTu6FejGmCW03ARdYK2t9W1JIhLuetO2t85DNTyaV8RLH5XR5HKzZPJwsrMymJk60O+1eNK2+AywEBhijCkDHqClqyUWeKe1xWaTtfZferBOEQkTvWXb28J9x8h17OXtTw8SHRnBygtSWDU/g/QhCQGryZMul5vbePixHqhFRKTDbW8DHehut+Xd3ZXkOPaypfgYA+Kj+fbXxvKNi9MY2i82oLWBVoqKSJDxx7a3XZ3SaWh28erWCnIce9l76CTJifE8cOUkbsgcRUJs8MRo8FQiIkLL9rblbYS3r7a97cqUzvG6Jp7eXMLjG4qoPNHApBH9eeSmGSybOoKoyODbrFaBLiJB5d7FE74SuODbbW89mdLZf7yOtflFPPNhKTUNzcwfN4Q/3DCDuWMHe7U0v6cp0EUkqPT0trcdTensPlBNrsPJa9sqsMDyaSNYNT+DKckDfHLtnqZAFxG/62wOe8XM5B67AdrelE5MVARL/phHfHQkt108mrvmpZMysE+P1NBTFOgi4levbC3n3he30+RqWY9YXlXHvS9uBzxvS/SmT72tKR2A6MgI7lk0llsvGk1in5gufEfBI/hm9UUkpP3i9Z1fhvkpTS7LL17f6dHrT93ULK+qw3L6puYrW8s9ev3iycNZNnXEl/uNR0YYbshMoeCnl/LtReN6bZiDRugi4mfHapu69PjZutunfvRkI+s2FrNu4z6OnmxkxqhE/mXBGC6blNStwySCkQJdRHqVrvaplxypZU2+k+cLSqlvcnPpecPIzhrDrLSBQd2x0h0KdBHxq8T4aKrqzh2NJ8Z7tqWsp33qH5dVkeNw8o9P9hMZYbhmZjKr5mcwzkeHSQQjzaGLiF/9/KrJRJ81xREdYfj5VZM9ev29iycQH/3VbWhP9alba3l/TyU3527iqj9twLHnENlZY/jpskls+OIIl/+Xg7kPvevxfHtvoxG6iPiVp33m7XWytPX67182Hre1XPFIHrsPnGB4/zh+svQ8brpwFP/cVdkrNvvyBePPrcwzMzNtQUGB364nIr3T2cvzoWUU/uC1U78SwjUNzTz7YQlr84uoOF7P+KS+ZGeN4arpI4mJapmAmPvQu21O0SQnxrPhvkU9/834gDGm0Fqb2dnzNEIXkU75a3/yU9dpK4DP7GSprK7n8Q+KeXLTPk7UN3NRxiB+dc1UFk4Yes6NTn9s9hUsFOgi0iF/7U/e1qj8bOVVdfzoxY/529Zymt1ulkwZTnbWGGaMSmz3NT292VcwUaCLSId8sT+5JyP8tq7T5nttK+eGWSncPS+DNA8Ok+jpzb6CiQJdRDrk7ZSFpyN8T95v8eQkfn3NVAb39fwwiZ7e7CuYKNBFpEPeTll4OsJv7zoAA+Kj+fHSidw4K7ULlZ/Wk5t9BRP1oYtIhzrq+/aEpyP8b31tDFFn96dHGn5//XS2P3B5t8M8nGiELiId8nbKorMRfnlVHY/lFfHslhKa3ZbYqAgamt2MHBDHD5dMDIuRta+oD11EelR7PeX3LBrL55U1vLa9AgNcOX0kq+ZnMGlk/8AVG6TUhy4iQeHMEX55VR2DE2IY2i+W3761h4SYSL45J40756WTHIJthP6mQBfxE38tzulp3fk+lk8bQUSEIdexlx3l1RhjuHfxBG6dPZoBfTzblEs6p0AX8QN/Lc7paV39Pmobm3l+Sylr8osoO1ZHxpAEHmpdvh931o1W8V6ngW6MWQssByqttVNaHxsEPAekAcXADdbaYz1Xpkjv5ovFOcHA0+/jSE0DT2zcx7qNxVTVNnHB6IH8+/JJXHpeEhEhcphEMPJkhP4X4E/AujMeuw/4p7X2IWPMfa1f/8j35YmEhlDZT6Sz76P48EnW5Dt5oaCMhmY3l01KYnVWBplpg/xZZtjqNNCttQ5jTNpZD18NLGz9/RPA+yjQRdoVKvuJtPd9DOkby78+Vcg/dhwgOiKCa89P5u75GYwd1jcAVYav7i4sSrLW7gdo/e8w35UkEnq8XZwTLNr6PiIMHKppIO/zw/yPBWPI/9HXeOi6aQrzAOjxm6LGmGwgGyA1VSu9JDydvTgnsU801sL3ntvGw2/t6TUdLytmJtPscvPLN3Z9eYxc/7hovr1oLDddmErfWPVZBFJ3P/2DxpgR1tr9xpgRQGV7T7TW5gK50LKwqJvXE+n1Tu0n0ls7Xk7UN/HMhyWszS+mqq6JicP7sXpBBsunjSQ6UruIBIPuBvprwO3AQ63/fdVnFYmEuN7W8XKwup61G4p4elMJJxqamTNmML9ZOY2scUPOOUxCAsuTtsVnaLkBOsQYUwY8QEuQP2+MuQsoAa7vySJFQklv6Xj5/OAJch1OXtlWjsttWTp1BKuzxjA1ZUCgS5N2eNLlcnM7f3SJj2sRCQvB3PFireXDoqPkOpz8c3clcdER3HJhKnfNyyB1cJ9Alyed0B0MET8LxhN0XG7LO58e4M/rnWwrrWJQQgzfu3Q8t108mkEJMQGrS7pGgS5hI1j2UgmmE3Tqm1y8WFjGmjwnxUdqSR3Uh/9cMYWV56cQH6Ol+b2NAl3CQrB1lnhygk5P/gCqqm3kxy9/wps7D+C2LQdJfHNOGj9bPolILc3vtRToEhZ6W2dJT/0AKj1ay2P5RTy9uYRGl/vLx5tclue2lDJjVGJQfh7iGQW6hIXe0llyiq9/AO0oP06uw8kbn+zHADGRETR+9e2D+geceEaBLmEhmDtL2uKLH0DWWvI+P0yuw0n+F4fpGxvFXfPSuWNuGnMefNfr95fgo0CXsBCMnSUd8eYHUJPLzRsf7yfH4WTX/mqG9YvlR0smcsvsVAbER3v9/hK8tF5XwsKKmck8eO1UkhPjMUByYjwPth60EIy6s5nXyYZm1uYXsfDh9/nuc9tocrm5adYooiIMv31zN0sfyeOVreXdfn8JfhqhS9jwpLMkWHSltfHQiQae+KCYv27ax/G6Ji5MG8QvrppMdX0TP/nbjg5vrAZD66T4jrHWf/tlZWZm2oKCAr9dTySUOQ/V8GheES99VEaTy83iScPJXpDB+akDAZj70LttTqskJ8az4b5F/i5XvGCMKbTWZnb2PI3QRXqZwn3HyHXs5e1PDxIdGcF156ewan46GUO/uv94b+vsEe8p0EW6IFCrTd1uy7u7K8lx7GVL8TEGxEfzrYVjuX1OGkP7xbb5Gt34DD8KdBEPBWK1aUOzi1e3VpDj2MveQydJTozn35dP4sZZo0jo5DCJ3tbZI95ToIt4yJ+rTY/XNfH05hIe31BE5YkGJo3ozyM3zWDp1BEeHyahG5/hR4Eu4iF/zEnvP17H2vwinvmwlJqGZuaNHcLvb5jOvLHdO0yiN3X2iPcU6CIe6sk56d0Hqsl1OHltWwUWWDZ1BNlZGUxJ1mES4jkFuoiHfD0nba1lo/MIuQ4n7+85RHx0JLdeNJq75qUzapAOk5CuU6CLeMhXc9Iut+XNHQfIcezl47LjDE6I4QeXjefWi0YzUIdJiBcU6CJd4M2cdF2jixcLS3k0r4iSo7WkD0ngV9dM4brzU4iL1mES4j0FukgnvO09P3qykXUbi1m3cR9HTzYyY1QiP146kcsmDddhEuJTCnSRDnjTe15ypJY1+U6eLyilvsnNJROHsXrBGGalDexWx4pIZxToIh3oTu/5x2VV5Dic/OOT/URGGFbMSCY7K4NxSf38UbKEMQW6SAc87T231rL+s0PkrHey0XmEfrFRrMrK4I456QwfEOePUkUU6CId6az3vMnl5vXtFeQ6nOw+cIKk/rH8eOlEbr4wlX5x0f4uV8KcAl2kA+31nt+zaCxr8pyszS+i4ng945P68rvrp3PV9JHEROncGAkMrwLdGPM94G7AAp8Ad1hr631RmEgwOLv3PKl/HJOT+/Or/97FifpmZqcP4pfXTGHh+GFEqGNFAqzbgW6MSQb+DZhkra0zxjwP3AT8xUe1iQSFFTOTmZI8gEcdTv62tZx3d1dyxZThZGeNYcaoxECXJ/Ilb6dcooB4Y0wT0Aeo8L4kkeBRUHyUHIeTdz49SGxUBDfMSuHueRmkDUkIdGki5+h2oFtry40xvwNKgDrgbWvt22c/zxiTDWQDpKamdvdyIn7jdlve2XWQXIeTwn3HSOwTzb9dMo5vXDyaIX3bPkxCJBh4M+UyELgaSAeqgBeMMbdaa58883nW2lwgF1rOFPWiVpEeVd/k4pWt5eTmOXEeOknKwHh+fuUkbpg1ij4x6h+Q4OfN39JLgSJr7SEAY8zLwBzgyQ5fJRJkjtc28eTmfTy+oZjDNQ1MSe7P/755JldMGU6Uh4dJiAQDbwK9BLjIGNOHlimXS4ACn1Ql4gflVacOkyihttFF1vihrM7KYM6YwVqaL72SN3Pom40xLwIfAc3AVlqnVkSC2a79LYdJvL695TCJq6aPZNX8DCaN7N/pawN1SLSIJ7yaGLTWPgA84KNaRHqMtZaNe4/wZ4cTx2eH6BMTye1z0rhzXjrJHp44FIhDokW6Qnd6JKQ1u9z8o/UwiR3l1QzpG8u9iydw6+zRDOjTtaX5/jwkWqQ7FOgSkmobm3mhoIw1+U5Kj9aRMSSBB6+dyjUzk7t9mIQ/DokW8YYCXULKkZoGnti4j3Ubi6mqbeL81ER+umwSl52X5PXS/J48JFrEFxToEhKKD59kTb6TFwrKaGh2c+l5SfzLggwy0wb57Bq+PiRaxNcU6NKrbS+tIsexlzd3HCAqIoJrZiazKiudscN8f5iErw6JFukpCnTpday1vL/nEH9ev5fNRUfpFxfF6gVjuGNOGsP69+xhEt4cEi3S0xTo0ms0Nrt5bXsFuY69fHawhhED4vjpsvO46cJU+sbqr7KI/hVI0Dl78c49i8ZSXd/E2vxiDlTXMyGpH3+4YTpXTh9JtJbmi3xJgS5Bpa3FO/e1Lt65OGMwD103lQXjh2ppvkgbFOgSVNpavAMwtG8sz2RfFICKRHoPBboEBWstW4qPtdnnDXC4pqHL76l9VyTcKNAloFxuyzufHiDH4WRrSRURBtxt7Jrf1cU72ndFwpECXXyiq6Ph+iYXL31Uxpq8IooOnyR1UB/+8+rJxEZF8sBrO71evKN9VyQcKdDFa10ZDVfVNvLkpn385YNiDtc0Mi1lAH+6ZSZXTBlBZOvS/JioCK+nSrTvioQjBbp4zZPRcNmxWh7LL+K5LaXUNrpYOGEoq7PGcFHGoHM6VnyxeEf7rkg4UqCL1zoaDe+sOE6uw8nfP96PAa6aMZLsrAwmDu/8MAlvaN8VCUcKdPFae6PhmKgIlv2vfBJiIrlzbhp3zE332whZ+65IOFKghyB/t+u1NRoGiI2K4LuXjueW2akMiO/aYRK+oH1XJNwo0ENMINr1Lp+cxPrPDvHatgpc1hIVYVh5QQq/aO1aERH/UKCHGH+26x2uaeCJD4pZt3Efx+uamJU2kOysMVwycZjXh0mISNcp0EOMP9r1ig6f5NE8Jy8WltHkcnP5pCSys8ZwweiBPruGiHSdAj0AenKOuyfb9baWHCNnvZO3Pj1AdGQE152fwt3z0xkztK/X7y0i3lOg+1lPz3H7ul3P7ba8t6eSnPVOPiw+Sv+4KL61cCy3z0ljaL9Yr+sVEd9RoPtZT89x+6pdr6HZxavbKsh1OPmisobkxHh+tnwSN84apcMkRIKUV/8yjTGJwBpgCmCBO621G31RWKjyxxy3N+161fVNPL25hLX5RVSeaOC8Ef35440zWDZthA6TEAly3g61HgHetNauNMbEAH18UFNIC9Yl6fuP1/H4hmKe3lxCTUMz88YO4XfXT2f+uCE6TEKkl+h2oBtj+gNZwDcBrLWNQKNvygpdwbYkfc+BE+Q6nLy2vRyX27Js2khWZ2UwJXlAQOoRke7zZoSeARwCHjfGTAcKge9Ya0+e+SRjTDaQDZCamurF5UJDMCxJt9ayuegoOev38t6eQ8RHR/L12aO5a146owbpf7JEeitjbRunCXjyQmMygU3AXGvtZmPMI0C1tfZn7b0mMzPTFhQUdK9S8ZrLbXlr5wFy1u9le9lxBifEcPucNG67aDQDE2L8VodOEhLpGmNMobU2s7PneTNCLwPKrLWbW79+EbjPi/eTHlLf5OKFwjLW5DnZd6SWtMF9+OWKKay8IIW4aP8uzddJQiI9p9uBbq09YIwpNcZMsNbuAS4BPvVdaeKtYycbWbdxH+s2FnPkZCPTRyVy35KJXD55+JeHSfibThIS6TnedrncAzzV2uHiBO7wviTxVunR04dJ1DW5WDRxGKuzMrgw/dzDJPxNJwmJ9ByvAt1auw3odF5H/GNH+XFyHE7e+LiCyAjD1TOSyc7KYHxSv0CX9qVgbdsUCQVa8tfLWWtxfH6YXMdeNnxxhL6xUayan8Edc9MZPiAu0OWdI9jaNkVCiQK9l2pyufn7xxXkOorYtb+apP6x3H/FRG6enUr/OP8fJuGpYGjbFAlVCvRe5mRDM89uKeWxPCcVx+sZN6wvD6+cxtUzkomJ6h1L83WSkEjPUKD3EpUn6nnig2L+unEf1fXNXJg+iF9eM4WF43WYhIi0UKAHub2HaliT5+SlwnKa3G6WTB5OdlYGM1N1mISIfJUCPUicvXpy5QUp7NpfzTu7DhIdGcH1mSncPT+D9CEJgS5VRIKUAj0ItLV68pF/fk6fmEju+dpYvjEnjSF9dZiEiHRMgR4Efvvm7nNWTwIMiI/m+5ernU9EPKNAD6DjdU08tXkfFcfr2/zzA+08LiLSFgV6AFRU1bE2v4hnPizhZKOL2KgIGprd5zxPqydFpCsU6H60+0A1ueudvLa9AgtcOW0Eq7Iy+PxgjVZPiojXFOg9zFrLRucRctY7Wf/ZIfrERHLbxS2HSaQMbDlMYvLIltOBtHpSRLyhQO8hzS43b+48QM56J5+UH2dI3xjuXTyBr89OJbHPuYdJaPWkiHhLge5jdY0uXigsZU1eESVHa8kYksCvr5nKtecn+/0wCREJLwp0HzlS0/DlYRLHapuYmZrIj5eex2WTkgJ2mISIhBcFupf2HTnJmrwiXigspb7JzaXnJbF6QQaZowcG/DAJEQkvCvRu2l5aRa7DyT927CcqIoJrZiazKiudscOC5zAJEQkvCvQusNby/meHyF3vZKPzCP3iosjOGsMdc9NI6h98h0mISHhRoHugsdnN69sreDTPye4DJxgxII6fLD2Pmy4cRb8gPkxCRMKLAr0DJ+qbePbDUtZuKGL/8XomJPXj99dP58rpI3vNYRIiEj4U6G2orK5n7YZintq8jxP1zVycMZhfXzuVheOH6kaniAQtBfoZvqis4VGHk79tLafZ7eaKKSPIzspg+qjEQJcmItKpsA90ay0F+46Rs97J/9t1kLjoCG6cNYq756czerAOkxCR3iNsA93ttrz96UFyHXv5qKSKgX2i+c4l4/jGxaMZrMMkRKQXCrtAr29y8fJH5azJc+I8fJJRg+L5j6snc/0Fo4iPCczS/LOPn9PGXCLSHV4HujEmEigAyq21y70vqWccr23iyc37eHxDMYdrGpiaPIA/3TKTJZOHExUZuI6Vto6fu//lTwAU6iLSJb4YoX8H2AX098F7+VzZsVrW5hfz7JYSahtdLBg/lNULMrg4Y3BQdKw8/Naec46fq2ty8fBbexToItIlXgW6MSYFWAb8Cvi+TyrykU8rqsl17OX1j/djgKumj2RVVgbnjQiunzsVVXVdelxEpD3ejtD/CPwQaHcDE2NMNpANkJqa6uXlOmatZcMXR8hx7CXv88MkxERyx5w07pyXHrTHuY1MjKe8jfAO1npFJHh1O9CNMcuBSmttoTFmYXvPs9bmArkAmZmZtrvX60izy81/7zhAzvq97KyoZmi/WH64ZAJfnz2aAfHBvTT/3sUTdPyciPiENyP0ucBVxpilQBzQ3xjzpLX2Vt+U1rnaxmae21LKY/lFlB2rI2NoAr+5biorZiYTG+WfjhVvO1ROPVddLiLiLWOt94Pm1hH6/+ysyyUzM9MWFBR4fb3DNQ2s+6CYdZv2UVXbRObogaxeMIZLJg4jwo+HSZzdoQIto+sHr52qQBYRnzHGFFprMzt7Xq/qQy8+fJJH85y8WFhGo8vNZa2HSVwwelBA6mmvQ+UHz28H1HYoIv7lk0C31r4PvO+L92rL1pJj5DqcvLnzANGREVx3fjJ3z89gzNC+PXVJj7TXieKyVr3kIuJ3QTtCd7st7+2pJMfh5MOio/SPi+JfF47h9jlpDOsXHIdJtNehAuolFxH/C7pAb2x28+q2cnIdTj6vrGHkgDh+tnwSN84aRd/Y4Cq3rQ6VM6mXXET8KWgSsrq+iWc2l7B2QxEHqxuYOLwff7xxBsumjSA6gEvzO3Jq9P2D57fjauPmsnrJRcSfAh7oB47X8/iGIp7aXEJNQzNzxw7m4ZXTmT9uSMCX5nvSknjqa/WSi0igBSzQPzt4glyHk1e3leNyW5ZNG8nqrAymJA8IVElf0ZVNs9RLLiLBwO+Bvtl5hByHk3d3VxIfHcnXZ4/mrnnpjBrUx9+ldKirm2atmJmsABeRgPJroH9RWcONuZsYnBDD9y8bz20XjWZgQow/S/CYNs0Skd7Gr4Huclt+uWIKKy9IIS46MIdJeEqbZolIb+PX9pEJw/tx60Wjgz7MoaUlMf6sOnWjU0SCWcC7XIKVbnSKSG+jQO+AbnSKSG8SnCt2RESkyxToIiIhQoEuIhIiFOgiIiFCgS4iEiIU6CIiIUKBLiISIhToIiIhQoEuIhIiFOgiIiFCgS4iEiIU6CIiIUKBLiISIrod6MaYUcaY94wxu4wxO40x3/FlYSIi0jXebJ/bDPzAWvuRMaYfUGiMecda+6mPahMRkS7o9gjdWrvfWvtR6+9PALsAbR4uIhIgPplDN8akATOBzW38WbYxpsAYU3Do0CFfXE5ERNrgdaAbY/oCLwHftdZWn/3n1tpca22mtTZz6NCh3l5ORETa4VWgG2OiaQnzp6y1L/umJBER6Q5vulwM8Biwy1r7B9+VJCIi3eHNCH0ucBuwyBizrfXXUh/VJSIiXdTttkVrbT5gfFiLiIh4QStFRURChAJdRCREKNBFREKEAl1EJEQo0EVEQoQCXUQkRCjQRURChAJdRCREKNBFREKEAl1EJEQo0EVEQoQCXUQkRCjQRURChAJdRCREKNBFREKEAl1EJEQo0EVEQoQCXUQkRCjQRURChAJdRCREKNBFREKEAl1EJEQo0EVEQoQCXUQkRCjQRURChFeBboxZYozZY4z5whhzn6+KEhGRrut2oBtjIoH/A1wBTAJuNsZM8lVhIiLSNd6M0C8EvrDWOq21jcCzwNW+KUtERLoqyovXJgOlZ3xdBsw++0nGmGwgu/XLBmPMDi+uGUqGAIcDXUSQ0Gdxmj6L0/RZnDbBkyd5E+imjcfsOQ9YmwvkAhhjCqy1mV5cM2ToszhNn8Vp+ixO02dxmjGmwJPneTPlUgaMOuPrFKDCi/cTEREveBPoW4Bxxph0Y0wMcBPwmm/KEhGRrur2lIu1ttkY823gLSASWGut3dnJy3K7e70QpM/iNH0Wp+mzOE2fxWkefRbG2nOmvUVEpBfSSlERkRChQBcRCRF+CXRtEXCaMWatMaYy3PvxjTGjjDHvGWN2GWN2GmO+E+iaAsUYE2eM+dAYs731s/hFoGsKNGNMpDFmqzHm74GuJZCMMcXGmE+MMds8aV3s8Tn01i0CPgMuo6XVcQtws7X20x69cJAyxmQBNcA6a+2UQNcTKMaYEcAIa+1Hxph+QCGwIhz/XhhjDJBgra0xxkQD+VaSNzgAAAHdSURBVMB3rLWbAlxawBhjvg9kAv2ttcsDXU+gGGOKgUxrrUcLrPwxQtcWAWew1jqAo4GuI9CstfuttR+1/v4EsIuW1cdhx7aoaf0yuvVX2HYrGGNSgGXAmkDX0tv4I9Db2iIgLP/hStuMMWnATGBzYCsJnNYphm1AJfCOtTZsPwvgj8APAXegCwkCFnjbGFPYuo1Kh/wR6B5tESDhyRjTF3gJ+K61tjrQ9QSKtdZlrZ1By4rrC40xYTkdZ4xZDlRaawsDXUuQmGutPZ+WXW2/1Tpl2y5/BLq2CJA2tc4XvwQ8Za19OdD1BANrbRXwPrAkwKUEylzgqta542eBRcaYJwNbUuBYayta/1sJ/I2WKex2+SPQtUWAnKP1RuBjwC5r7R8CXU8gGWOGGmMSW38fD1wK7A5sVYFhrb3fWptirU2jJSvetdbeGuCyAsIYk9DaMIAxJgG4HOiwO67HA91a2wyc2iJgF/C8B1sEhCxjzDPARmCCMabMGHNXoGsKkLnAbbSMwLa1/loa6KICZATwnjHmY1oGQO9Ya8O6XU8ASALyjTHbgQ+BN6y1b3b0Ai39FxEJEVopKiISIhToIiIhQoEuIhIiFOgiIiFCgS4iEiIU6CIiIUKBLiISIv4/MKHshMuSKVAAAAAASUVORK5CYII=\n",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<!-- Created with matplotlib (https://matplotlib.org/) -->\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.10625 248.518125\" width=\"372.10625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <defs>\r\n  <style type=\"text/css\">\r\n*{stroke-linecap:butt;stroke-linejoin:round;}\r\n  </style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M -0 248.518125 \r\nL 372.10625 248.518125 \r\nL 372.10625 0 \r\nL -0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\nL 361.725 7.2 \r\nL 26.925 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"PathCollection_1\">\r\n    <defs>\r\n     <path d=\"M 0 3 \r\nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \r\nC 2.683901 1.55874 3 0.795609 3 0 \r\nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \r\nC 1.55874 -2.683901 0.795609 -3 0 -3 \r\nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \r\nC -2.683901 -1.55874 -3 -0.795609 -3 0 \r\nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \r\nC -1.55874 2.683901 -0.795609 3 0 3 \r\nz\r\n\" id=\"m6cb25fac81\" style=\"stroke:#1f77b4;\"/>\r\n    </defs>\r\n    <g clip-path=\"url(#peb18b9499b)\">\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"296.823081\" xlink:href=\"#m6cb25fac81\" y=\"43.681492\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"95.375023\" xlink:href=\"#m6cb25fac81\" y=\"120.621527\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"148.134091\" xlink:href=\"#m6cb25fac81\" y=\"111.899809\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"268.069708\" xlink:href=\"#m6cb25fac81\" y=\"45.435577\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"296.257969\" xlink:href=\"#m6cb25fac81\" y=\"29.569039\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"233.401885\" xlink:href=\"#m6cb25fac81\" y=\"67.827267\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"100.103511\" xlink:href=\"#m6cb25fac81\" y=\"137.678399\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"304.393291\" xlink:href=\"#m6cb25fac81\" y=\"37.37272\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"224.348476\" xlink:href=\"#m6cb25fac81\" y=\"60.891289\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"288.378383\" xlink:href=\"#m6cb25fac81\" y=\"37.799242\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"119.421244\" xlink:href=\"#m6cb25fac81\" y=\"116.885357\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"158.94773\" xlink:href=\"#m6cb25fac81\" y=\"127.358249\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"293.835578\" xlink:href=\"#m6cb25fac81\" y=\"45.73913\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"83.491263\" xlink:href=\"#m6cb25fac81\" y=\"165.213675\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"81.750473\" xlink:href=\"#m6cb25fac81\" y=\"162.810882\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"189.403877\" xlink:href=\"#m6cb25fac81\" y=\"66.652773\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"185.275487\" xlink:href=\"#m6cb25fac81\" y=\"109.817813\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"257.267468\" xlink:href=\"#m6cb25fac81\" y=\"68.15505\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"316.045306\" xlink:href=\"#m6cb25fac81\" y=\"43.1547\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"250.223607\" xlink:href=\"#m6cb25fac81\" y=\"72.854113\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"319.979089\" xlink:href=\"#m6cb25fac81\" y=\"32.572127\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"203.045261\" xlink:href=\"#m6cb25fac81\" y=\"84.777313\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"167.166833\" xlink:href=\"#m6cb25fac81\" y=\"106.43053\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"314.27596\" xlink:href=\"#m6cb25fac81\" y=\"15.127239\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"283.964116\" xlink:href=\"#m6cb25fac81\" y=\"47.459064\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"287.620468\" xlink:href=\"#m6cb25fac81\" y=\"36.665295\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"271.344332\" xlink:href=\"#m6cb25fac81\" y=\"54.736766\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"221.391099\" xlink:href=\"#m6cb25fac81\" y=\"54.35679\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"98.720761\" xlink:href=\"#m6cb25fac81\" y=\"157.119784\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"73.115889\" xlink:href=\"#m6cb25fac81\" y=\"169.360144\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.431461\" xlink:href=\"#m6cb25fac81\" y=\"98.17309\"/>\r\n     <use style=\"fill:#1f77b4;stroke:#1f77b4;\" x=\"206.428252\" xlink:href=\"#m6cb25fac81\" y=\"76.512832\"/>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m3cecbd2400\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#m3cecbd2400\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 66.40625 \r\nQ 24.171875 66.40625 20.328125 58.90625 \r\nQ 16.5 51.421875 16.5 36.375 \r\nQ 16.5 21.390625 20.328125 13.890625 \r\nQ 24.171875 6.390625 31.78125 6.390625 \r\nQ 39.453125 6.390625 43.28125 13.890625 \r\nQ 47.125 21.390625 47.125 36.375 \r\nQ 47.125 51.421875 43.28125 58.90625 \r\nQ 39.453125 66.40625 31.78125 66.40625 \r\nz\r\nM 31.78125 74.21875 \r\nQ 44.046875 74.21875 50.515625 64.515625 \r\nQ 56.984375 54.828125 56.984375 36.375 \r\nQ 56.984375 17.96875 50.515625 8.265625 \r\nQ 44.046875 -1.421875 31.78125 -1.421875 \r\nQ 19.53125 -1.421875 13.0625 8.265625 \r\nQ 6.59375 17.96875 6.59375 36.375 \r\nQ 6.59375 54.828125 13.0625 64.515625 \r\nQ 19.53125 74.21875 31.78125 74.21875 \r\nz\r\n\" id=\"DejaVuSans-48\"/>\r\n      </defs>\r\n      <g transform=\"translate(23.74375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"93.885\" xlink:href=\"#m3cecbd2400\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 1 -->\r\n      <defs>\r\n       <path d=\"M 12.40625 8.296875 \r\nL 28.515625 8.296875 \r\nL 28.515625 63.921875 \r\nL 10.984375 60.40625 \r\nL 10.984375 69.390625 \r\nL 28.421875 72.90625 \r\nL 38.28125 72.90625 \r\nL 38.28125 8.296875 \r\nL 54.390625 8.296875 \r\nL 54.390625 0 \r\nL 12.40625 0 \r\nz\r\n\" id=\"DejaVuSans-49\"/>\r\n      </defs>\r\n      <g transform=\"translate(90.70375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"160.845\" xlink:href=\"#m3cecbd2400\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 2 -->\r\n      <defs>\r\n       <path d=\"M 19.1875 8.296875 \r\nL 53.609375 8.296875 \r\nL 53.609375 0 \r\nL 7.328125 0 \r\nL 7.328125 8.296875 \r\nQ 12.9375 14.109375 22.625 23.890625 \r\nQ 32.328125 33.6875 34.8125 36.53125 \r\nQ 39.546875 41.84375 41.421875 45.53125 \r\nQ 43.3125 49.21875 43.3125 52.78125 \r\nQ 43.3125 58.59375 39.234375 62.25 \r\nQ 35.15625 65.921875 28.609375 65.921875 \r\nQ 23.96875 65.921875 18.8125 64.3125 \r\nQ 13.671875 62.703125 7.8125 59.421875 \r\nL 7.8125 69.390625 \r\nQ 13.765625 71.78125 18.9375 73 \r\nQ 24.125 74.21875 28.421875 74.21875 \r\nQ 39.75 74.21875 46.484375 68.546875 \r\nQ 53.21875 62.890625 53.21875 53.421875 \r\nQ 53.21875 48.921875 51.53125 44.890625 \r\nQ 49.859375 40.875 45.40625 35.40625 \r\nQ 44.1875 33.984375 37.640625 27.21875 \r\nQ 31.109375 20.453125 19.1875 8.296875 \r\nz\r\n\" id=\"DejaVuSans-50\"/>\r\n      </defs>\r\n      <g transform=\"translate(157.66375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"227.805\" xlink:href=\"#m3cecbd2400\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 3 -->\r\n      <defs>\r\n       <path d=\"M 40.578125 39.3125 \r\nQ 47.65625 37.796875 51.625 33 \r\nQ 55.609375 28.21875 55.609375 21.1875 \r\nQ 55.609375 10.40625 48.1875 4.484375 \r\nQ 40.765625 -1.421875 27.09375 -1.421875 \r\nQ 22.515625 -1.421875 17.65625 -0.515625 \r\nQ 12.796875 0.390625 7.625 2.203125 \r\nL 7.625 11.71875 \r\nQ 11.71875 9.328125 16.59375 8.109375 \r\nQ 21.484375 6.890625 26.8125 6.890625 \r\nQ 36.078125 6.890625 40.9375 10.546875 \r\nQ 45.796875 14.203125 45.796875 21.1875 \r\nQ 45.796875 27.640625 41.28125 31.265625 \r\nQ 36.765625 34.90625 28.71875 34.90625 \r\nL 20.21875 34.90625 \r\nL 20.21875 43.015625 \r\nL 29.109375 43.015625 \r\nQ 36.375 43.015625 40.234375 45.921875 \r\nQ 44.09375 48.828125 44.09375 54.296875 \r\nQ 44.09375 59.90625 40.109375 62.90625 \r\nQ 36.140625 65.921875 28.71875 65.921875 \r\nQ 24.65625 65.921875 20.015625 65.03125 \r\nQ 15.375 64.15625 9.8125 62.3125 \r\nL 9.8125 71.09375 \r\nQ 15.4375 72.65625 20.34375 73.4375 \r\nQ 25.25 74.21875 29.59375 74.21875 \r\nQ 40.828125 74.21875 47.359375 69.109375 \r\nQ 53.90625 64.015625 53.90625 55.328125 \r\nQ 53.90625 49.265625 50.4375 45.09375 \r\nQ 46.96875 40.921875 40.578125 39.3125 \r\nz\r\n\" id=\"DejaVuSans-51\"/>\r\n      </defs>\r\n      <g transform=\"translate(224.62375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-51\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"294.765\" xlink:href=\"#m3cecbd2400\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 4 -->\r\n      <defs>\r\n       <path d=\"M 37.796875 64.3125 \r\nL 12.890625 25.390625 \r\nL 37.796875 25.390625 \r\nz\r\nM 35.203125 72.90625 \r\nL 47.609375 72.90625 \r\nL 47.609375 25.390625 \r\nL 58.015625 25.390625 \r\nL 58.015625 17.1875 \r\nL 47.609375 17.1875 \r\nL 47.609375 0 \r\nL 37.796875 0 \r\nL 37.796875 17.1875 \r\nL 4.890625 17.1875 \r\nL 4.890625 26.703125 \r\nz\r\n\" id=\"DejaVuSans-52\"/>\r\n      </defs>\r\n      <g transform=\"translate(291.58375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"361.725\" xlink:href=\"#m3cecbd2400\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 5 -->\r\n      <defs>\r\n       <path d=\"M 10.796875 72.90625 \r\nL 49.515625 72.90625 \r\nL 49.515625 64.59375 \r\nL 19.828125 64.59375 \r\nL 19.828125 46.734375 \r\nQ 21.96875 47.46875 24.109375 47.828125 \r\nQ 26.265625 48.1875 28.421875 48.1875 \r\nQ 40.625 48.1875 47.75 41.5 \r\nQ 54.890625 34.8125 54.890625 23.390625 \r\nQ 54.890625 11.625 47.5625 5.09375 \r\nQ 40.234375 -1.421875 26.90625 -1.421875 \r\nQ 22.3125 -1.421875 17.546875 -0.640625 \r\nQ 12.796875 0.140625 7.71875 1.703125 \r\nL 7.71875 11.625 \r\nQ 12.109375 9.234375 16.796875 8.0625 \r\nQ 21.484375 6.890625 26.703125 6.890625 \r\nQ 35.15625 6.890625 40.078125 11.328125 \r\nQ 45.015625 15.765625 45.015625 23.390625 \r\nQ 45.015625 31 40.078125 35.4375 \r\nQ 35.15625 39.890625 26.703125 39.890625 \r\nQ 22.75 39.890625 18.8125 39.015625 \r\nQ 14.890625 38.140625 10.796875 36.28125 \r\nz\r\n\" id=\"DejaVuSans-53\"/>\r\n      </defs>\r\n      <g transform=\"translate(358.54375 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-53\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mecab21489b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(13.5625 228.439219)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"191.187692\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 2 -->\r\n      <g transform=\"translate(13.5625 194.986911)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"157.735385\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 4 -->\r\n      <g transform=\"translate(13.5625 161.534603)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-52\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"124.283077\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 6 -->\r\n      <defs>\r\n       <path d=\"M 33.015625 40.375 \r\nQ 26.375 40.375 22.484375 35.828125 \r\nQ 18.609375 31.296875 18.609375 23.390625 \r\nQ 18.609375 15.53125 22.484375 10.953125 \r\nQ 26.375 6.390625 33.015625 6.390625 \r\nQ 39.65625 6.390625 43.53125 10.953125 \r\nQ 47.40625 15.53125 47.40625 23.390625 \r\nQ 47.40625 31.296875 43.53125 35.828125 \r\nQ 39.65625 40.375 33.015625 40.375 \r\nz\r\nM 52.59375 71.296875 \r\nL 52.59375 62.3125 \r\nQ 48.875 64.0625 45.09375 64.984375 \r\nQ 41.3125 65.921875 37.59375 65.921875 \r\nQ 27.828125 65.921875 22.671875 59.328125 \r\nQ 17.53125 52.734375 16.796875 39.40625 \r\nQ 19.671875 43.65625 24.015625 45.921875 \r\nQ 28.375 48.1875 33.59375 48.1875 \r\nQ 44.578125 48.1875 50.953125 41.515625 \r\nQ 57.328125 34.859375 57.328125 23.390625 \r\nQ 57.328125 12.15625 50.6875 5.359375 \r\nQ 44.046875 -1.421875 33.015625 -1.421875 \r\nQ 20.359375 -1.421875 13.671875 8.265625 \r\nQ 6.984375 17.96875 6.984375 36.375 \r\nQ 6.984375 53.65625 15.1875 63.9375 \r\nQ 23.390625 74.21875 37.203125 74.21875 \r\nQ 40.921875 74.21875 44.703125 73.484375 \r\nQ 48.484375 72.75 52.59375 71.296875 \r\nz\r\n\" id=\"DejaVuSans-54\"/>\r\n      </defs>\r\n      <g transform=\"translate(13.5625 128.082296)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-54\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"90.830769\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 8 -->\r\n      <defs>\r\n       <path d=\"M 31.78125 34.625 \r\nQ 24.75 34.625 20.71875 30.859375 \r\nQ 16.703125 27.09375 16.703125 20.515625 \r\nQ 16.703125 13.921875 20.71875 10.15625 \r\nQ 24.75 6.390625 31.78125 6.390625 \r\nQ 38.8125 6.390625 42.859375 10.171875 \r\nQ 46.921875 13.96875 46.921875 20.515625 \r\nQ 46.921875 27.09375 42.890625 30.859375 \r\nQ 38.875 34.625 31.78125 34.625 \r\nz\r\nM 21.921875 38.8125 \r\nQ 15.578125 40.375 12.03125 44.71875 \r\nQ 8.5 49.078125 8.5 55.328125 \r\nQ 8.5 64.0625 14.71875 69.140625 \r\nQ 20.953125 74.21875 31.78125 74.21875 \r\nQ 42.671875 74.21875 48.875 69.140625 \r\nQ 55.078125 64.0625 55.078125 55.328125 \r\nQ 55.078125 49.078125 51.53125 44.71875 \r\nQ 48 40.375 41.703125 38.8125 \r\nQ 48.828125 37.15625 52.796875 32.3125 \r\nQ 56.78125 27.484375 56.78125 20.515625 \r\nQ 56.78125 9.90625 50.3125 4.234375 \r\nQ 43.84375 -1.421875 31.78125 -1.421875 \r\nQ 19.734375 -1.421875 13.25 4.234375 \r\nQ 6.78125 9.90625 6.78125 20.515625 \r\nQ 6.78125 27.484375 10.78125 32.3125 \r\nQ 14.796875 37.15625 21.921875 38.8125 \r\nz\r\nM 18.3125 54.390625 \r\nQ 18.3125 48.734375 21.84375 45.5625 \r\nQ 25.390625 42.390625 31.78125 42.390625 \r\nQ 38.140625 42.390625 41.71875 45.5625 \r\nQ 45.3125 48.734375 45.3125 54.390625 \r\nQ 45.3125 60.0625 41.71875 63.234375 \r\nQ 38.140625 66.40625 31.78125 66.40625 \r\nQ 25.390625 66.40625 21.84375 63.234375 \r\nQ 18.3125 60.0625 18.3125 54.390625 \r\nz\r\n\" id=\"DejaVuSans-56\"/>\r\n      </defs>\r\n      <g transform=\"translate(13.5625 94.629988)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-56\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"57.378462\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(7.2 61.17768)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"26.925\" xlink:href=\"#mecab21489b\" y=\"23.926154\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 12 -->\r\n      <g transform=\"translate(7.2 27.725373)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-49\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-50\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#peb18b9499b)\" d=\"M 26.925 171.135712 \r\nL 93.885 140.387018 \r\nL 160.845 109.638329 \r\nL 227.805 78.889624 \r\nL 294.765 48.140934 \r\nL 361.725 17.392245 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 26.925 224.64 \r\nL 26.925 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 361.725 224.64 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 26.925 224.64 \r\nL 361.725 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 26.925 7.2 \r\nL 361.725 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"peb18b9499b\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"26.925\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "text/plain": "<Figure size 432x288 with 1 Axes>"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "w:2.0740339756011963b:3.0318779945373535\n"
    }
   ],
   "source": [
    "# 随机初始化参数\n",
    "w = t.rand(1, 1, dtype=t.float).to(device)\n",
    "b = t.zeros(1, 1, dtype=t.float).to(device)\n",
    "\n",
    "lr =0.02 # 学习率\n",
    "\n",
    "for ii in range(500):\n",
    "    x, y = get_fake_data(batch_size=4)\n",
    "    # forward：计算loss\n",
    "    y_pred = x.mm(w) + b.expand_as(y) # x@W等价于x.mm(w);for python3 only\n",
    "    loss = 0.5 * (y_pred - y) ** 2 # 均方误差\n",
    "    loss = loss.mean()\n",
    "    \n",
    "    # backward：手动计算梯度\n",
    "    dloss = 1\n",
    "    dy_pred = dloss * (y_pred - y)\n",
    "    \n",
    "    dw = x.t().mm(dy_pred)\n",
    "    db = dy_pred.sum()\n",
    "    \n",
    "    # 更新参数\n",
    "    w.sub_(lr * dw)\n",
    "    b.sub_(lr * db)\n",
    "    \n",
    "    if ii%50 ==0:\n",
    "       \n",
    "        # 画图\n",
    "        display.clear_output(wait=True)\n",
    "        x = t.arange(0, 6).view(-1, 1).float()\n",
    "        y = x.mm(w) + b.expand_as(x)\n",
    "        plt.plot(x.cpu().numpy(), y.cpu().numpy()) # predicted\n",
    "        \n",
    "        x2, y2 = get_fake_data(batch_size=32) \n",
    "        plt.scatter(x2.numpy(), y2.numpy()) # true data\n",
    "        \n",
    "        plt.xlim(0, 5)\n",
    "        plt.ylim(0, 13)\n",
    "        plt.show()\n",
    "        plt.pause(0.5)\n",
    "        \n",
    "print('w: ', w.item(), 'b: ', b.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "可见程序已经基本学出w=2、b=3，并且图中直线和数据已经实现较好的拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "虽然上面提到了许多操作，但是只要掌握了这个例子基本上就可以了，其他的知识，读者日后遇到的时候，可以再看看这部份的内容或者查找对应文档。\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}